hydra:
  run:
    dir: . # prevent outputs from training to go to hydra sub-folders

experiment:
  name: NER
  group: ${data.language}-${model.type}
  do:
      train: true
      finetune: false
      eval: true
      predict: true
  evaluate_during_training: true
  evaluate_all_checkpoints: true

data:
  path: data/${data.language}/cap-${data.cap}
  labels: #
  cap: 1
  eval_cap: 2
  max_seq_length: 256
  per_gpu_train_batch_size: 32
  per_gpu_eval_batch_size: 8
  eval_batch_size:  # SET IN SCRIPT, DO NOT REMOVE
  train_batch_size: # SET IN SCRIPT, DO NOT REMOVE

model:
  save_steps: 100 # Save checkpoint every X updates steps.
  input_dir:
  output_dir: results/${model.name_or_path}/${data.language}/cap-${data.cap}
  overwrite_output_dir: true
  do_lower_case: true # Set this flag if you are using an uncased model.
  cache_dir: # "Where do you want to store the pre-trained models downloaded from s3"
  overwrite_cache: true
  config_name: # Pretrained config name or path if not the same as model_name
  tokenizer_name: # Pretrained tokenizer name or path if not the same as model_name"

optim:
  num_train_epochs: 30
  learning_rate: 5e-5
  weight_decay: 0.0
  adam_epsilon: 1e-8
  max_grad_norm: 1.0
  max_steps: -1
  warmup_steps: 0
  gradient_accumulation_steps: 1

logging:
  wandb: 
    use: true
    offline: true
  steps: 50

device:
  server_ip: 
  server_port: 
  local_rank: -1
  no_cuda: false
  seed: 42
  n_gpu: # # SET IN SCRIPT, DO NOT REMOVE
  backend: # SET IN SCRIPT, DO NOT REMOVE
