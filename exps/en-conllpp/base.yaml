hydra:
  run:
    dir: . # prevent outputs from training to go to hydra sub-folders

experiment:
  do:
      train: true
      finetune: false
      eval: true
      predict: true
  evaluate_during_training: true
  evaluate_all_checkpoints: true

data:
  path: data/${data.language}/cap-${data.cap}
  language: en-conllpp # kin, swa, pcm, conllpp
  labels: #
  cap: 1
  max_seq_length: 256
  per_gpu_train_batch_size: 32
  per_gpu_eval_batch_size: 8
  train_batch_size: # SET IN SCRIPT, DO NOT REMOVE

model:
  save_steps: 10000 # Save checkpoint every X updates steps.

optim:
  num_train_epochs: 30
  learning_rate: 5e-5
  weight_decay: 0.0
  adam_epsilon: 1e-8
  max_grad_norm: 1.0
  max_steps: -1
  warmup_steps: 0
  gradient_accumulation_steps: 1

logging:
  wandb: 
    use: true
    offline: true
  steps: 500

device:
  server_ip: 
  server_port: 
  local_rank: -1
  no_cuda: false
  seed: 42
  n_gpu: # # SET IN SCRIPT, DO NOT REMOVE
  backend: # SET IN SCRIPT, DO NOT REMOVE
