@article{masakhaNER_old,
  author    = {David Ifeoluwa Adelani and
               Jade Z. Abbott and
               Graham Neubig and
               Daniel D'souza and
               Julia Kreutzer and
               Constantine Lignos and
               Chester Palen{-}Michel and
               Happy Buzaaba and
               Shruti Rijhwani and
               Sebastian Ruder and
               Stephen Mayhew and
               Israel Abebe Azime and
               Shamsuddeen Hassan Muhammad and
               Chris Chinenye Emezue and
               Joyce Nakatumba{-}Nabende and
               Perez Ogayo and
               Aremu Anuoluwapo and
               Catherine Gitau and
               Derguene Mbaye and
               Jesujoba O. Alabi and
               Seid Muhie Yimam and
               Tajuddeen Gwadabe and
               Ignatius Ezeani and
               Rubungo Andre Niyongabo and
               Jonathan Mukiibi and
               Verrah Otiende and
               Iroro Orife and
               Davis David and
               Samba Ngom and
               Tosin P. Adewumi and
               Paul Rayson and
               Mofetoluwa Adeyemi and
               Gerald Muriuki and
               Emmanuel Anebi and
               Chiamaka Chukwuneke and
               Nkiruka Odu and
               Eric Peter Wairagala and
               Samuel Oyerinde and
               Clemencia Siro and
               Tobius Saul Bateesa and
               Temilola Oloyede and
               Yvonne Wambui and
               Victor Akinode and
               Deborah Nabagereka and
               Maurice Katusiime and
               Ayodele Awokoya and
               Mouhamadane Mboup and
               Dibora Gebreyohannes and
               Henok Tilaye and
               Kelechi Nwaike and
               Degaga Wolde and
               Abdoulaye Faye and
               Blessing Sibanda and
               Orevaoghene Ahia and
               Bonaventure F. P. Dossou and
               Kelechi Ogueji and
               Thierno Ibrahima Diop and
               Abdoulaye Diallo and
               Adewale Akinfaderin and
               Tendai Marengereke and
               Salomey Osei},
  title     = {MasakhaNER: Named Entity Recognition for African Languages},
  journal   = {CoRR},
  volume    = {abs/2103.11811},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.11811},
  eprinttype = {arXiv},
  eprint    = {2103.11811},
  timestamp = {Thu, 14 Oct 2021 09:13:45 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-11811.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{masakhaNER,
  author    = {David Ifeoluwa Adelani and
               Jade Z. Abbott and
               Graham Neubig and
               Daniel D'souza and
               Julia Kreutzer and
               Constantine Lignos and
               Chester Palen{-}Michel and
               Happy Buzaaba and
               Shruti Rijhwani and
               Sebastian Ruder and
               Stephen Mayhew and
               Israel Abebe Azime and
               Shamsuddeen Hassan Muhammad and
               Chris Chinenye Emezue and
               Joyce Nakatumba{-}Nabende and
               Perez Ogayo and
               Aremu Anuoluwapo and
               Catherine Gitau and
               Derguene Mbaye and
               Jesujoba O. Alabi and
               Seid Muhie Yimam and
               Tajuddeen Gwadabe and
               Ignatius Ezeani and
               Rubungo Andre Niyongabo and
               Jonathan Mukiibi and
               Verrah Otiende and
               Iroro Orife and
               Davis David and
               Samba Ngom and
               Tosin P. Adewumi and
               Paul Rayson and
               Mofetoluwa Adeyemi and
               Gerald Muriuki and
               Emmanuel Anebi and
               Chiamaka Chukwuneke and
               Nkiruka Odu and
               Eric Peter Wairagala and
               Samuel Oyerinde and
               Clemencia Siro and
               Tobius Saul Bateesa and
               Temilola Oloyede and
               Yvonne Wambui and
               Victor Akinode and
               Deborah Nabagereka and
               Maurice Katusiime and
               Ayodele Awokoya and
               Mouhamadane Mboup and
               Dibora Gebreyohannes and
               Henok Tilaye and
               Kelechi Nwaike and
               Degaga Wolde and
               Abdoulaye Faye and
               Blessing Sibanda and
               Orevaoghene Ahia and
               Bonaventure F. P. Dossou and
               Kelechi Ogueji and
               Thierno Ibrahima Diop and
               Abdoulaye Diallo and
               Adewale Akinfaderin and
               Tendai Marengereke and
               Salomey Osei},
  title     = {MasakhaNER: Named Entity Recognition for African Languages},
  journal = {Transactions of the Association for Computational Linguistics},
    volume = {9},
    pages = {1116-1131},
    year = {2021},
    month = {10},
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00416},
    url = {https://doi.org/10.1162/tacl\_a\_00416},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00416/1966201/tacl\_a\_00416.pdf},
}

@article{masakhaNER_bad,
    author = {Adelani, David Ifeoluwa and Abbott, Jade and Neubig, Graham and Dâ€™souza, Daniel and Kreutzer, Julia and Lignos, Constantine and Palen-Michel, Chester and Buzaaba, Happy and Rijhwani, Shruti and Ruder, Sebastian and Mayhew, Stephen and Azime, Israel Abebe and Muhammad, Shamsuddeen H. and Emezue, Chris Chinenye and Nakatumba-Nabende, Joyce and Ogayo, Perez and Anuoluwapo, Aremu and Gitau, Catherine and Mbaye, Derguene and Alabi, Jesujoba and Yimam, Seid Muhie and Gwadabe, Tajuddeen Rabiu and Ezeani, Ignatius and Niyongabo, Rubungo Andre and Mukiibi, Jonathan and Otiende, Verrah and Orife, Iroro and David, Davis and Ngom, Samba and Adewumi, Tosin and Rayson, Paul and Adeyemi, Mofetoluwa and Muriuki, Gerald and Anebi, Emmanuel and Chukwuneke, Chiamaka and Odu, Nkiruka and Wairagala, Eric Peter and Oyerinde, Samuel and Siro, Clemencia and Bateesa, Tobius Saul and Oloyede, Temilola and Wambui, Yvonne and Akinode, Victor and Nabagereka, Deborah and Katusiime, Maurice and Awokoya, Ayodele and MBOUP, Mouhamadane and Gebreyohannes, Dibora and Tilaye, Henok and Nwaike, Kelechi and Wolde, Degaga and Faye, Abdoulaye and Sibanda, Blessing and Ahia, Orevaoghene and Dossou, Bonaventure F. P. and Ogueji, Kelechi and DIOP, Thierno Ibrahima and Diallo, Abdoulaye and Akinfaderin, Adewale and Marengereke, Tendai and Osei, Salomey},
    title = "{MasakhaNER: Named Entity Recognition for African Languages}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {9},
    pages = {1116-1131},
    year = {2021},
    month = {10},
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00416},
    url = {https://doi.org/10.1162/tacl\_a\_00416},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00416/1966201/tacl\_a\_00416.pdf},
}

@inproceedings{zero_shot_transfer,
  author    = {Tsung{-}Yuan Hsu and
               Chi{-}Liang Liu and
               Hung{-}yi Lee},
  editor    = {Kentaro Inui and
               Jing Jiang and
               Vincent Ng and
               Xiaojun Wan},
  title     = {Zero-shot Reading Comprehension by Cross-lingual Transfer Learning
               with Multi-lingual Language Representation Model},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural
               Language Processing and the 9th International Joint Conference on
               Natural Language Processing, {EMNLP-IJCNLP} 2019, Hong Kong, China,
               November 3-7, 2019},
  pages     = {5932--5939},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
  url       = {https://doi.org/10.18653/v1/D19-1607},
  doi       = {10.18653/v1/D19-1607},
  timestamp = {Thu, 05 Aug 2021 17:36:17 +0200},
  biburl    = {https://dblp.org/rec/conf/emnlp/HsuLL19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  annote={
    Does in effect what we try, i.e. zero shot transfer, just on RC tasks (e.g. Squad). 
    
    Section 3 has some details about what the models learn.
    
    
    mBert does not totally rely on pattern matching.

    They have a very cool picture of PCA BERT embeddings before and after finetuning.

    Pretty cool. Could do similar stuff just for NER on african languages.
  }
} 

@inproceedings{syntactic_knowledge_transfer,
  author    = {Prajit Dhar and
               Arianna Bisazza},
  editor    = {Tal Linzen and
               Grzegorz Chrupala and
               Afra Alishahi},
  title     = {Does Syntactic Knowledge in Multilingual Language Models Transfer
               Across Languages?},
  booktitle = {Proceedings of the Workshop: Analyzing and Interpreting Neural Networks
               for NLP, BlackboxNLP@EMNLP 2018, Brussels, Belgium, November 1, 2018},
  pages     = {374--377},
  publisher = {Association for Computational Linguistics},
  year      = {2018},
  url       = {https://doi.org/10.18653/v1/w18-5453},
  doi       = {10.18653/v1/w18-5453},
  timestamp = {Fri, 06 Aug 2021 00:40:25 +0200},
  biburl    = {https://dblp.org/rec/conf/emnlp/DharB18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},


  annote={
    Don't really find syntactic transfer from french to italian.
  } 
}


@inproceedings{choosing_transfer_languages,
  author    = {Yu{-}Hsiang Lin and
               Chian{-}Yu Chen and
               Jean Lee and
               Zirui Li and
               Yuyan Zhang and
               Mengzhou Xia and
               Shruti Rijhwani and
               Junxian He and
               Zhisong Zhang and
               Xuezhe Ma and
               Antonios Anastasopoulos and
               Patrick Littell and
               Graham Neubig},
  editor    = {Anna Korhonen and
               David R. Traum and
               Llu{\'{\i}}s M{\`{a}}rquez},
  title     = {Choosing Transfer Languages for Cross-Lingual Learning},
  booktitle = {Proceedings of the 57th Conference of the Association for Computational
               Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
               Volume 1: Long Papers},
  pages     = {3125--3135},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
  url       = {https://doi.org/10.18653/v1/p19-1301},
  doi       = {10.18653/v1/p19-1301},
  timestamp = {Fri, 06 Aug 2021 00:41:01 +0200},
  biburl    = {https://dblp.org/rec/conf/acl/LinCLLZXRHZMALN19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  annote={
    The question is what languages to transfer from?


    They do cite some things that use transfer learning to improve NER !!! named entity recognition (Zhang et al., 2016; Mayhew et al., 2017; Xie et al., 2018),


    TLDR, many features impact transfer learning performance.
  }
}

% More bib things:
@article{interpreting_ner,
  author    = {Oshin Agarwal and
               Yinfei Yang and
               Byron C. Wallace and
               Ani Nenkova},
  title     = {Interpretability Analysis for Named Entity Recognition to Understand
               System Predictions and How They Can Improve},
  journal   = {Comput. Linguistics},
  volume    = {47},
  number    = {1},
  pages     = {117--140},
  year      = {2021},
  url       = {https://doi.org/10.1162/coli\_a\_00397},
  doi       = {10.1162/coli\_a\_00397},
  timestamp = {Thu, 20 May 2021 16:32:27 +0200},
  biburl    = {https://dblp.org/rec/journals/coling/AgarwalYWN21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
  ,annote={
    TLDR, do NER systems use context, or are they merely memorising the words themselves? That is the main question.


    NER struggles usually when training on unseen entities.


    Their experiment is quite clever, in that they use either only words or only context and see what the models predict when given only one. They find that the main reason is that the words are in effect being remembered.

    "The number of examples for which relying primarily on contextual features will result
    in an accurate prediction is almost the same as the number for which relying on the
    context will lead to an erroneous prediction"

    Gazetteers are in effect large lookup tables that list common names and such.


    Only words is better than only contexts, but both are better still.

    Their discussion is pretty neat.
  }
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@inproceedings{sang2003introduction_conll,
  author    = {Erik F. Tjong Kim Sang and
               Fien De Meulder},
  editor    = {Walter Daelemans and
               Miles Osborne},
  title     = {Introduction to the CoNLL-2003 Shared Task: Language-Independent Named
               Entity Recognition},
  booktitle = {Proceedings of the Seventh Conference on Natural Language Learning,
               CoNLL 2003, Held in cooperation with {HLT-NAACL} 2003, Edmonton, Canada,
               May 31 - June 1, 2003},
  pages     = {142--147},
  publisher = {{ACL}},
  year      = {2003},
  url       = {https://aclanthology.org/W03-0419/},
  timestamp = {Fri, 06 Aug 2021 00:41:09 +0200},
  biburl    = {https://dblp.org/rec/conf/conll/SangM03.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

  @inproceedings{arches_for_ner,
  author    = {Guillaume Lample and
               Miguel Ballesteros and
               Sandeep Subramanian and
               Kazuya Kawakami and
               Chris Dyer},
  editor    = {Kevin Knight and
               Ani Nenkova and
               Owen Rambow},
  title     = {Neural Architectures for Named Entity Recognition},
  booktitle = {{NAACL} {HLT} 2016, The 2016 Conference of the North American Chapter
               of the Association for Computational Linguistics: Human Language Technologies,
               San Diego California, USA, June 12-17, 2016},
  pages     = {260--270},
  publisher = {The Association for Computational Linguistics},
  year      = {2016},
  url       = {https://doi.org/10.18653/v1/n16-1030},
  doi       = {10.18653/v1/n16-1030},
  timestamp = {Fri, 06 Aug 2021 00:41:32 +0200},
  biburl    = {https://dblp.org/rec/conf/naacl/LampleBSKD16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{marrero2013named,
  title={Named entity recognition: fallacies, challenges and opportunities},
  author={Marrero, M{\'o}nica and Urbano, Juli{\'a}n and S{\'a}nchez-Cuadrado, Sonia and Morato, Jorge and G{\'o}mez-Berb{\'\i}s, Juan Miguel},
  journal={Computer Standards \& Interfaces},
  volume={35},
  number={5},
  pages={482--489},
  year={2013},
  publisher={Elsevier}
}




@article{radford2018improving_gpt,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={2018}
}
@article{radford2019language_gpt2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{brown2020language_gpt3,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}



% Transfer Learning
@article{americas_nli,
  author    = {Abteen Ebrahimi and
               Manuel Mager and
               Arturo Oncevay and
               Vishrav Chaudhary and
               Luis Chiruzzo and
               Angela Fan and
               John Ortega and
               Ricardo Ramos and
               Annette Rios and
               Ivan Vladimir and
               Gustavo A. Gim{\'{e}}nez{-}Lugo and
               Elisabeth Mager and
               Graham Neubig and
               Alexis Palmer and
               Rolando A. Coto Solano and
               Ngoc Thang Vu and
               Katharina Kann},
  title     = {AmericasNLI: Evaluating Zero-shot Natural Language Understanding of
               Pretrained Multilingual Models in Truly Low-resource Languages},
  journal   = {CoRR},
  volume    = {abs/2104.08726},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.08726},
  eprinttype = {arXiv},
  eprint    = {2104.08726},
  timestamp = {Mon, 26 Apr 2021 17:25:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-08726.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  annote={
    They say zero shot is bad for languages not in training set. They say most investigations into this has been on low level tasks (like NER), and they focus on higher level.

    Nice related work section.


    Seems cool: Additional work has been done examining what mechanisms allow multilingual models to transfer across languages (Pires et al., 2019; Wu and
Dredze, 2019). Wu and Dredze (2020) examine
transfer performance dependent on a languageâ€™s
representation in the pretraining data



TLDR: Somewhat cool, don't really say much though

  }
}

@article{survey_transfer_nlp,
  author    = {Zaid Alyafeai and
               Maged Saeed AlShaibani and
               Irfan Ahmad},
  title     = {A Survey on Transfer Learning in Natural Language Processing},
  journal   = {CoRR},
  volume    = {abs/2007.04239},
  year      = {2020},
  url       = {https://arxiv.org/abs/2007.04239},
  eprinttype = {arXiv},
  eprint    = {2007.04239},
  timestamp = {Wed, 02 Jun 2021 08:22:38 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2007-04239.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  annote={
    Not properly published...

    Section 6.1.1 work section on cross lingual things.
    Nice quote: 
    For instance, cross-lingual
language modelling has been studied to see the effect on low-resource languages (Adams et al., 2017).
  }
}

@inproceedings{cross_lingual_transfer,
  author    = {Sebastian Schuster and
               Sonal Gupta and
               Rushin Shah and
               Mike Lewis},
  editor    = {Jill Burstein and
               Christy Doran and
               Thamar Solorio},
  title     = {Cross-lingual Transfer Learning for Multilingual Task Oriented Dialog},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of
               the Association for Computational Linguistics: Human Language Technologies,
               {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
               and Short Papers)},
  pages     = {3795--3805},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
  url       = {https://doi.org/10.18653/v1/n19-1380},
  doi       = {10.18653/v1/n19-1380},
  timestamp = {Fri, 06 Aug 2021 00:41:31 +0200},
  biburl    = {https://dblp.org/rec/conf/naacl/SchusterGSL19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{what_special_bert_layers,
  author    = {Wietse de Vries and
               Andreas van Cranenburgh and
               Malvina Nissim},
  editor    = {Trevor Cohn and
               Yulan He and
               Yang Liu},
  title     = {What's so special about BERT's layers? {A} closer look at the {NLP}
               pipeline in monolingual and multilingual models},
  booktitle = {Findings of the Association for Computational Linguistics: {EMNLP}
               2020, Online Event, 16-20 November 2020},
  series    = {Findings of {ACL}},
  volume    = {{EMNLP} 2020},
  pages     = {4339--4350},
  publisher = {Association for Computational Linguistics},
  year      = {2020},
  url       = {https://doi.org/10.18653/v1/2020.findings-emnlp.389},
  doi       = {10.18653/v1/2020.findings-emnlp.389},
  timestamp = {Fri, 27 Aug 2021 08:39:19 +0200},
  biburl    = {https://dblp.org/rec/conf/emnlp/VriesCN20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  annote={
    Introduction says what analysis has been done (maybe can be for ideas).

    Cite this: Nevertheless, mBERT performs surprisingly well on zero-shot POS tagging and Named
Entity Recognition (NER), as well as on crosslingual model transfer (Pires et al., 2019).

  Kinda interesting. Could probably do something like: analyse different NER tokens.
  }
}


@inproceedings{bert_multilingual_abc_analyse,
  author    = {Isabel Papadimitriou and
               Ethan A. Chi and
               Richard Futrell and
               Kyle Mahowald},
  editor    = {Paola Merlo and
               J{\"{o}}rg Tiedemann and
               Reut Tsarfaty},
  title     = {Deep Subjecthood: Higher-Order Grammatical Features in Multilingual
               {BERT}},
  booktitle = {Proceedings of the 16th Conference of the European Chapter of the
               Association for Computational Linguistics: Main Volume, {EACL} 2021,
               Online, April 19 - 23, 2021},
  pages     = {2522--2532},
  publisher = {Association for Computational Linguistics},
  year      = {2021},
  url       = {https://aclanthology.org/2021.eacl-main.215/},
  timestamp = {Fri, 06 Aug 2021 00:40:45 +0200},
  biburl    = {https://dblp.org/rec/conf/eacl/PapadimitriouCF21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  annote={
  TODO

  Good Question: By training a classifier on one language and testing on others, we can ask: is subjecthood
encoded in parallel ways across languages in
mBERT space?
  }
}

@inproceedings{ruder2019transfer,
  title={Transfer Learning in Natural Language Processing},
  author={Ruder, Sebastian and Peters, Matthew E and Swayamdipta, Swabha and Wolf, Thomas},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials},
  pages={15--18},
  year={2019}
}

@inproceedings{soa_best,
  author    = {Haoming Jiang and
               Pengcheng He and
               Weizhu Chen and
               Xiaodong Liu and
               Jianfeng Gao and
               Tuo Zhao},
  editor    = {Dan Jurafsky and
               Joyce Chai and
               Natalie Schluter and
               Joel R. Tetreault},
  title     = {{SMART:} Robust and Efficient Fine-Tuning for Pre-trained Natural
               Language Models through Principled Regularized Optimization},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational
               Linguistics, {ACL} 2020, Online, July 5-10, 2020},
  pages     = {2177--2190},
  publisher = {Association for Computational Linguistics},
  year      = {2020},
  url       = {https://doi.org/10.18653/v1/2020.acl-main.197},
  doi       = {10.18653/v1/2020.acl-main.197},
  timestamp = {Thu, 14 Oct 2021 09:46:04 +0200},
  biburl    = {https://dblp.org/rec/conf/acl/JiangHCLGZ20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{math_dataset_soa,
  author    = {Dan Hendrycks and
               Collin Burns and
               Saurav Kadavath and
               Akul Arora and
               Steven Basart and
               Eric Tang and
               Dawn Song and
               Jacob Steinhardt},
  title     = {Measuring Mathematical Problem Solving With the {MATH} Dataset},
  journal   = {CoRR},
  volume    = {abs/2103.03874},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.03874},
  eprinttype = {arXiv},
  eprint    = {2103.03874},
  timestamp = {Mon, 15 Mar 2021 17:30:55 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-03874.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{t5_model,
  author    = {Colin Raffel and
               Noam Shazeer and
               Adam Roberts and
               Katherine Lee and
               Sharan Narang and
               Michael Matena and
               Yanqi Zhou and
               Wei Li and
               Peter J. Liu},
  title     = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
               Transformer},
  journal   = {J. Mach. Learn. Res.},
  volume    = {21},
  pages     = {140:1--140:67},
  year      = {2020},
  url       = {http://jmlr.org/papers/v21/20-074.html},
  timestamp = {Fri, 05 Feb 2021 15:43:41 +0100},
  biburl    = {https://dblp.org/rec/journals/jmlr/RaffelSRLNMZLL20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{bert_og,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  editor    = {Jill Burstein and
               Christy Doran and
               Thamar Solorio},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of
               the Association for Computational Linguistics: Human Language Technologies,
               {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
               and Short Papers)},
  pages     = {4171--4186},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
  url       = {https://doi.org/10.18653/v1/n19-1423},
  doi       = {10.18653/v1/n19-1423},
  timestamp = {Fri, 06 Aug 2021 00:41:31 +0200},
  biburl    = {https://dblp.org/rec/conf/naacl/DevlinCLT19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{devries2019bertje,
	title = {{BERTje}: {A} {Dutch} {BERT} {Model}},
	shorttitle = {{BERTje}},
	author = {de Vries, Wietse  and  van Cranenburgh, Andreas  and  Bisazza, Arianna  and  Caselli, Tommaso  and  Noord, Gertjan van  and  Nissim, Malvina},
	year = {2019},
	month = dec,
	howpublished = {arXiv:1912.09582},
	url = {http://arxiv.org/abs/1912.09582},
}


@inproceedings{mt5,
  author    = {Linting Xue and
               Noah Constant and
               Adam Roberts and
               Mihir Kale and
               Rami Al{-}Rfou and
               Aditya Siddhant and
               Aditya Barua and
               Colin Raffel},
  editor    = {Kristina Toutanova and
               Anna Rumshisky and
               Luke Zettlemoyer and
               Dilek Hakkani{-}T{\"{u}}r and
               Iz Beltagy and
               Steven Bethard and
               Ryan Cotterell and
               Tanmoy Chakraborty and
               Yichao Zhou},
  title     = {mT5: {A} Massively Multilingual Pre-trained Text-to-Text Transformer},
  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of
               the Association for Computational Linguistics: Human Language Technologies,
               {NAACL-HLT} 2021, Online, June 6-11, 2021},
  pages     = {483--498},
  publisher = {Association for Computational Linguistics},
  year      = {2021},
  url       = {https://doi.org/10.18653/v1/2021.naacl-main.41},
  doi       = {10.18653/v1/2021.naacl-main.41},
  timestamp = {Fri, 06 Aug 2021 00:41:32 +0200},
  biburl    = {https://dblp.org/rec/conf/naacl/XueCRKASBR21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{romanian_bert,
  author    = {Mihai Masala and
               Stefan Ruseti and
               Mihai Dascalu},
  editor    = {Donia Scott and
               N{\'{u}}ria Bel and
               Chengqing Zong},
  title     = {RoBERT - {A} Romanian {BERT} Model},
  booktitle = {Proceedings of the 28th International Conference on Computational
               Linguistics, {COLING} 2020, Barcelona, Spain (Online), December 8-13,
               2020},
  pages     = {6626--6637},
  publisher = {International Committee on Computational Linguistics},
  year      = {2020},
  url       = {https://doi.org/10.18653/v1/2020.coling-main.581},
  doi       = {10.18653/v1/2020.coling-main.581},
  timestamp = {Fri, 06 Aug 2021 00:39:47 +0200},
  biburl    = {https://dblp.org/rec/conf/coling/MasalaRD20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{canete2020spanish,
  title={Spanish pre-trained bert model and evaluation data},
  author={Canete, Jos{\'e} and Chaperon, Gabriel and Fuentes, Rodrigo and Ho, Jou-Hui and Kang, Hojin and P{\'e}rez, Jorge},
  journal={Pml4dc at iclr},
  volume={2020},
  pages={2020},
  year={2020}
}

@inproceedings{howard-ruder-2018-universal,
    title = "Universal Language Model Fine-tuning for Text Classification",
    author = "Howard, Jeremy  and
      Ruder, Sebastian",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1031",
    doi = "10.18653/v1/P18-1031",
    pages = "328--339",
    abstract = "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24{\%} on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",
}

@misc{ruder2021lmfine-tuning,
  author = {Ruder, Sebastian},
  title = {{Recent Advances in Language Model Fine-tuning}},
  year = {2021},
  howpublished = {\url{http://ruder.io/recent-advances-lm-fine-tuning}},
}

@article{palen2021seqscore_arxiv,
  title={SeqScore: Addressing Barriers to Reproducible Named Entity Recognition Evaluation},
  author={Palen-Michel, Chester and Holley, Nolan and Lignos, Constantine},
  journal={arXiv preprint arXiv:2107.14154},
  year={2021}
}

@inproceedings{palen2021seqscore,
    title = "{S}eq{S}core: Addressing Barriers to Reproducible Named Entity Recognition Evaluation",
    author = "Palen-Michel, Chester  and
      Holley, Nolan  and
      Lignos, Constantine",
    booktitle = "Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eval4nlp-1.5",
    pages = "40--50",
}

@book{borg2005modern,
  title={Modern multidimensional scaling: Theory and applications},
  author={Borg, Ingwer and Groenen, Patrick JF},
  year={2005},
  publisher={Springer Science \& Business Media}
}


@InProceedings{word_embeddings_low_resource_alabi,
  author    = {Alabi, Jesujoba  and  Amponsah-Kaakyire, Kwabena  and  Adelani, David  and  EspaÃ±a-Bonet, Cristina},
title = {Massive vs. Curated Word Embeddings for Low-Resourced Languages. The
Case of {Y}or{\`{u}}b{\'{a}} and {T}wi},
  booktitle      = {Proceedings of The 12th Language Resources and Evaluation Conference},
  month          = {May},
  year           = {2020},
  address        = {Marseille, France},
  publisher      = {European Language Resources Association},
  pages     = {2754--2762},
  url       = {https://www.aclweb.org/anthology/2020.lrec-1.335}
}



@article{word_embeddings_low_resource_alabi_old,
author = {Jesujoba O. Alabi and
Kwabena Amponsah{-}Kaakyire and
David Ifeoluwa Adelani and
Cristina Espa{\~{n}}a{-}Bonet},
title = {Massive vs. Curated Word Embeddings for Low-Resourced Languages. The
Case of {Y}or{\`{u}}b{\'{a}} and {T}wi},
journal = {CoRR},
volume = {abs/1912.02481},
year = {2019},
url = {http://arxiv.org/abs/1912.02481},
archivePrefix = {arXiv},
eprint = {1912.02481},
timestamp = {Thu, 02 Jan 2020 18:08:18 +0100},
biburl = {https://dblp.org/rec/journals/corr/abs-1912-02481.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{adapters_nlp,
  author    = {Neil Houlsby and
               Andrei Giurgiu and
               Stanislaw Jastrzebski and
               Bruna Morrone and
               Quentin de Laroussilhe and
               Andrea Gesmundo and
               Mona Attariyan and
               Sylvain Gelly},
  editor    = {Kamalika Chaudhuri and
               Ruslan Salakhutdinov},
  title     = {Parameter-Efficient Transfer Learning for {NLP}},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning,
               {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
  pages     = {2790--2799},
  publisher = {{PMLR}},
  year      = {2019},
  url       = {http://proceedings.mlr.press/v97/houlsby19a.html},
  timestamp = {Tue, 11 Jun 2019 15:37:38 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/HoulsbyGJMLGAG19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{arabic,
author = {Versteegh, Kees},
year = {2001},
month = {12},
pages = {470-508},
title = {Linguistic Contacts Between Arabic and Other Languages},
volume = {48},
journal = {Arabica},
doi = {10.1163/157005801323163825}
}

@inproceedings{madx,
  author    = {Jonas Pfeiffer and
               Ivan Vulic and
               Iryna Gurevych and
               Sebastian Ruder},
  editor    = {Bonnie Webber and
               Trevor Cohn and
               Yulan He and
               Yang Liu},
  title     = {{MAD-X:} An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural
               Language Processing, {EMNLP} 2020, Online, November 16-20, 2020},
  pages     = {7654--7673},
  publisher = {Association for Computational Linguistics},
  year      = {2020},
  url       = {https://doi.org/10.18653/v1/2020.emnlp-main.617},
  doi       = {10.18653/v1/2020.emnlp-main.617},
  timestamp = {Fri, 06 Aug 2021 00:40:26 +0200},
  biburl    = {https://dblp.org/rec/conf/emnlp/PfeifferVGR20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{mann_whitney_test,
author = {H. B. Mann and D. R. Whitney},
title = {{On a Test of Whether one of Two Random Variables is Stochastically Larger than the Other}},
volume = {18},
journal = {The Annals of Mathematical Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {50 -- 60},
year = {1947},
doi = {10.1214/aoms/1177730491},
URL = {https://doi.org/10.1214/aoms/1177730491}
}

@article{shapiro1965analysis,
  title={An analysis of variance test for normality (complete samples)},
  author={Shapiro, Samuel Sanford and Wilk, Martin B},
  journal={Biometrika},
  volume={52},
  number={3/4},
  pages={591--611},
  year={1965},
  publisher={JSTOR}
}

@inproceedings{bender2020climbing,
  title={Climbing towards NLU: On meaning, form, and understanding in the age of data},
  author={Bender, Emily M and Koller, Alexander},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={5185--5198},
  year={2020}
}

@inproceedings{stoch_parrot,
author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? },
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445922},
doi = {10.1145/3442188.3445922},
abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {610â€“623},
numpages = {14},
location = {Virtual Event, Canada},
series = {FAccT '21}
}



@article{kleinberg2017netanos,
  title={Netanos-named entity-based text anonymization for open science},
  author={Kleinberg, Bennett and Mozes, Maximilian and van der Toolen, Yaloe and others},
  year={2017},
  publisher={OSF Preprints}
}
@inproceedings{hassan2018anonymization,
  title={Anonymization of unstructured data via named-entity recognition},
  author={Hassan, Fadi and Domingo-Ferrer, Josep and Soria-Comas, Jordi},
  booktitle={International conference on modeling decisions for artificial intelligence},
  pages={296--305},
  year={2018},
  organization={Springer}
}

@inproceedings{bias_in_embeddings,
  author    = {Yi Chern Tan and
               L. Elisa Celis},
  editor    = {Hanna M. Wallach and
               Hugo Larochelle and
               Alina Beygelzimer and
               Florence d'Alch{\'{e}}{-}Buc and
               Emily B. Fox and
               Roman Garnett},
  title     = {Assessing Social and Intersectional Biases in Contextualized Word
               Representations},
  booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
               on Neural Information Processing Systems 2019, NeurIPS 2019, December
               8-14, 2019, Vancouver, BC, Canada},
  pages     = {13209--13220},
  year      = {2019},
  url       = {https://proceedings.neurips.cc/paper/2019/hash/201d546992726352471cfea6b0df0a48-Abstract.html},
  timestamp = {Thu, 21 Jan 2021 15:15:20 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/TanC19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{ruder2020beyondenglish,
  author = {Ruder, Sebastian},
  title = {{Why You Should Do NLP Beyond English}},
  year = {2020},
  howpublished = {\url{http://ruder.io/nlp-beyond-english}},
}

@article{van2008visualizing,
  title={Visualizing data using t-SNE.},
  author={Van der Maaten, Laurens and Hinton, Geoffrey},
  journal={Journal of machine learning research},
  volume={9},
  number={11},
  year={2008}
}


@inproceedings{cross_lingual_transfer_bert,
  author    = {Karthikeyan K and
               Zihan Wang and
               Stephen Mayhew and
               Dan Roth},
  title     = {Cross-Lingual Ability of Multilingual {BERT:} An Empirical Study},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  year      = {2020},
  url       = {https://openreview.net/forum?id=HJeT3yrtDr},
  timestamp = {Thu, 04 Jun 2020 12:32:21 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/KWMR20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  annote={
    This investigates multilingual BERT and how it transfers to other languages, and what affects this transfer.

   Their results show that transfer occurs even when there is 0 token overlap.


   The gist is that word-piece tokenising is much better than word or character tokenising. Also, NSP seems to hurt performance.

   \begin{quote}
   Our contributions are threefold: (i) we provide the first extensive study of the aspects of M-BERT
that give rise to its cross-lingual ability; (ii) we develop a methodology that facilitates the analysis of
similarities between languages and their impact on cross-lingual models; we do this by mapping English to a Fake-English language, that is identical in all aspects to English but shares no word-pieces
with any target language; finally, (iii) we develop a set of insights into B-BERT, along linguistic,
architectural, and learning dimensions, that would contribute to further understanding and to the
development of more advanced cross-lingual neural models.
\end{quote}


  They found that word-piece overlap has a negligible performance impact. Word order is much more relevant. Structural similarity has a large effect. Larger models transfer better. WordPiece is much better overall.



  Pretty solid overall, quite interesting. Would be nice to see works that this cited.
  
  }
}


@inproceedings{afriberta,
    title = "Small Data? No Problem! Exploring the Viability of Pretrained Multilingual Language Models for Low-resourced Languages",
    author = "Ogueji, Kelechi  and
      Zhu, Yuxin  and
      Lin, Jimmy",
    booktitle = "Proceedings of the 1st Workshop on Multilingual Representation Learning",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.mrl-1.11",
    pages = "116--126",
    abstract = "Pretrained multilingual language models have been shown to work well on many languages for a variety of downstream NLP tasks. However, these models are known to require a lot of training data. This consequently leaves out a huge percentage of the world{'}s languages as they are under-resourced. Furthermore, a major motivation behind these models is that lower-resource languages benefit from joint training with higher-resource languages. In this work, we challenge this assumption and present the first attempt at training a multilingual language model on only low-resource languages. We show that it is possible to train competitive multilingual language models on less than 1 GB of text. Our model, named AfriBERTa, covers 11 African languages, including the first language model for 4 of these languages. Evaluations on named entity recognition and text classification spanning 10 languages show that our model outperforms mBERT and XLM-Rin several languages and is very competitive overall. Results suggest that our {``}small data{''} approach based on similar languages may sometimes work better than joint training on large datasets with high-resource languages. Code, data and models are released at https://github.com/keleog/afriberta.",

    annote={AfriBERTa. The gist is that traditional transformers, like BERT require very much training data, hindering applicability to many languages that have a scarcity of data.
    
    The results show that they do pretty well, only on low-resourecd languages \& very small datasets.


    They show that only using low-resourced languages works, and gives competitive results. We do not need high-resourced languages to transfer from. Using less than 1GB of text is valid. Indicate some important factors.


    Their background section is quite detailed and descriptive, of other approaches that used very little data to train large language models (seemingly going against common intuition).


    Their \textbf{total} dataset size is less than 1GB - for all languages.

    They use MLM as the objective, and do \textbf{not} use NSP.


    Very competitive with XLM-Roberta, while having much faster training, less params, etc.

    Very cool paper.
    }
}

@inproceedings{adaptive_finetuning,
    title = "Don{'}t Stop Pretraining: Adapt Language Models to Domains and Tasks",
    author = "Gururangan, Suchin  and
      Marasovi{\'c}, Ana  and
      Swayamdipta, Swabha  and
      Lo, Kyle  and
      Beltagy, Iz  and
      Downey, Doug  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.740",
    doi = "10.18653/v1/2020.acl-main.740",
    pages = "8342--8360",
    abstract = "Language models pretrained on text from a wide variety of sources form the foundation of today{'}s NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task{'}s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.",
}



@inproceedings{tran2019ZeroShot,
  author    = {Ke M. Tran and
               Arianna Bisazza},
  editor    = {Colin Cherry and
               Greg Durrett and
               George F. Foster and
               Reza Haffari and
               Shahram Khadivi and
               Nanyun Peng and
               Xiang Ren and
               Swabha Swayamdipta},
  title     = {Zero-shot Dependency Parsing with Pre-trained Multilingual Sentence
               Representations},
  booktitle = {Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource
               NLP, DeepLo@EMNLP-IJCNLP 2019, Hong Kong, China, November 3, 2019},
  pages     = {281--288},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
  url       = {https://doi.org/10.18653/v1/D19-6132},
  doi       = {10.18653/v1/D19-6132},
  timestamp = {Thu, 05 Aug 2021 17:36:17 +0200},
  biburl    = {https://dblp.org/rec/conf/acl-deeplo/TranB19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}




@inproceedings{pires2019How,
  author    = {Telmo Pires and
               Eva Schlinger and
               Dan Garrette},
  editor    = {Anna Korhonen and
               David R. Traum and
               Llu{\'{\i}}s M{\`{a}}rquez},
  title     = {How Multilingual is Multilingual BERT?},
  booktitle = {Proceedings of the 57th Conference of the Association for Computational
               Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
               Volume 1: Long Papers},
  pages     = {4996--5001},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
  url       = {https://doi.org/10.18653/v1/p19-1493},
  doi       = {10.18653/v1/p19-1493},
  timestamp = {Fri, 06 Aug 2021 00:41:01 +0200},
  biburl    = {https://dblp.org/rec/conf/acl/PiresSG19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}





@article{Dolicki2021Analysing,
  author    = {Blazej Dolicki and
               Gerasimos Spanakis},
  title     = {Analysing The Impact Of Linguistic Features On Cross-Lingual Transfer},
  journal   = {CoRR},
  volume    = {abs/2105.05975},
  year      = {2021},
  url       = {https://arxiv.org/abs/2105.05975},
  eprinttype = {arXiv},
  eprint    = {2105.05975},
  timestamp = {Tue, 18 May 2021 18:46:40 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-05975.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zhou2021Closer,
  author    = {Yichu Zhou and
               Vivek Srikumar},
  editor    = {Smaranda Muresan and
               Preslav Nakov and
               Aline Villavicencio},
  title     = {A Closer Look at How Fine-tuning Changes {BERT}},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational
               Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin, Ireland,
               May 22-27, 2022},
  pages     = {1046--1061},
  publisher = {Association for Computational Linguistics},
  year      = {2022},
  url       = {https://doi.org/10.18653/v1/2022.acl-long.75},
  doi       = {10.18653/v1/2022.acl-long.75},
  timestamp = {Mon, 01 Aug 2022 16:27:43 +0200},
  biburl    = {https://dblp.org/rec/conf/acl/ZhouS22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{muller2020When,
  author    = {Benjamin M{\"{u}}ller and
               Antonios Anastasopoulos and
               Beno{\^{\i}}t Sagot and
               Djam{\'{e}} Seddah},
  title     = {When Being Unseen from mBERT is just the Beginning: Handling New Languages
               With Multilingual Language Models},
  booktitle = {{NAACL-HLT}},
  pages     = {448--462},
  publisher = {Association for Computational Linguistics},
  year      = {2021}
}



@inproceedings{malkin2022Balanced,
  author    = {Dan Malkin and
               Tomasz Limisiewicz and
               Gabriel Stanovsky},
  editor    = {Marine Carpuat and
               Marie{-}Catherine de Marneffe and
               Iv{\'{a}}n Vladimir Meza Ru{\'{\i}}z},
  title     = {A Balanced Data Approach for Evaluating Cross-Lingual Transfer: Mapping
               the Linguistic Blood Bank},
  booktitle = {Proceedings of the 2022 Conference of the North American Chapter of
               the Association for Computational Linguistics: Human Language Technologies,
               {NAACL} 2022, Seattle, WA, United States, July 10-15, 2022},
  pages     = {4903--4915},
  publisher = {Association for Computational Linguistics},
  year      = {2022},
  url       = {https://doi.org/10.18653/v1/2022.naacl-main.361},
  doi       = {10.18653/v1/2022.naacl-main.361},
  timestamp = {Mon, 01 Aug 2022 16:27:59 +0200},
  biburl    = {https://dblp.org/rec/conf/naacl/MalkinLS22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}