[
  {"id":"abdulmuminSeparatingGrainsChaff2022","abstract":"We participated in the WMT 2022 Large-Scale Machine Translation Evaluation for the African Languages Shared Task. This work describes our approach, which is based on filtering the given noisy data using a sentence-pair classifier that was built by fine-tuning a pre-trained language model. To train the classifier, we obtain positive samples (i.e. high-quality parallel sentences) from a gold-standard curated dataset and extract negative samples (i.e. low-quality parallel sentences) from automatically aligned parallel data by choosing sentences with low alignment scores. Our final machine translation model was then trained on filtered data, instead of the entire noisy dataset. We empirically validate our approach by evaluating on two common datasets and show that data filtering generally improves overall translation quality, in some cases even significantly.","accessed":{"date-parts":[[2022,11,29]]},"author":[{"family":"Abdulmumin","given":"Idris"},{"family":"Beukman","given":"Michael"},{"family":"Alabi","given":"Jesujoba O."},{"family":"Emezue","given":"Chris"},{"family":"Asiko","given":"Everlyn"},{"family":"Adewumi","given":"Tosin"},{"family":"Muhammad","given":"Shamsuddeen Hassan"},{"family":"Adeyemi","given":"Mofetoluwa"},{"family":"Yousuf","given":"Oreen"},{"family":"Singh","given":"Sahib"},{"family":"Gwadabe","given":"Tajuddeen Rabiu"}],"citation-key":"abdulmuminSeparatingGrainsChaff2022","issued":{"date-parts":[[2022,10,20]]},"number":"arXiv:2210.10692","publisher":"arXiv","source":"arXiv.org","title":"Separating Grains from the Chaff: Using Data Filtering to Improve Multilingual Translation for Low-Resourced African Languages","title-short":"Separating Grains from the Chaff","type":"article","URL":"http://arxiv.org/abs/2210.10692"},
  {"id":"alabiMassiveVsCurated2020","abstract":"The success of several architectures to learn semantic representations from unannotated text and the availability of these kind of texts in online multilingual resources such as Wikipedia has facilitated the massive and automatic creation of resources for multiple languages. The evaluation of such resources is usually done for the high-resourced languages, where one has a smorgasbord of tasks and test sets to evaluate on. For low-resourced languages, the evaluation is more difficult and normally ignored, with the hope that the impressive capability of deep learning architectures to learn (multilingual) representations in the high-resourced setting holds in the low-resourced setting too. In this paper we focus on two African languages, Yor\\`ub\\'a and Twi, and compare the word embeddings obtained in this way, with word embeddings obtained from curated corpora and a language-dependent processing. We analyse the noise in the publicly available corpora, collect high quality and noisy data for the two languages and quantify the improvements that depend not only on the amount of data but on the quality too. We also use different architectures that learn word representations both from surface forms and characters to further exploit all the available information which showed to be important for these languages. For the evaluation, we manually translate the wordsim-353 word pairs dataset from English into Yor\\`ub\\'a and Twi. As output of the work, we provide corpora, embeddings and the test suits for both languages.","accessed":{"date-parts":[[2022,11,29]]},"author":[{"family":"Alabi","given":"Jesujoba O."},{"family":"Amponsah-Kaakyire","given":"Kwabena"},{"family":"Adelani","given":"David I."},{"family":"Espa√±a-Bonet","given":"Cristina"}],"citation-key":"alabiMassiveVsCurated2020","issued":{"date-parts":[[2020,3,28]]},"number":"arXiv:1912.02481","publisher":"arXiv","source":"arXiv.org","title":"Massive vs. Curated Word Embeddings for Low-Resourced Languages. The Case of Yor\\`ub\\'a and Twi","type":"article","URL":"http://arxiv.org/abs/1912.02481"},
  {"id":"jieBetterModelingIncomplete2019a","accessed":{"date-parts":[[2022,11,20]]},"author":[{"family":"Jie","given":"Zhanming"},{"family":"Xie","given":"Pengjun"},{"family":"Lu","given":"Wei"},{"family":"Ding","given":"Ruixue"},{"family":"Li","given":"Linlin"}],"citation-key":"jieBetterModelingIncomplete2019a","container-title":"Proceedings of the 2019 Conference of the North","DOI":"10.18653/v1/N19-1079","event-place":"Minneapolis, Minnesota","event-title":"Proceedings of the 2019 Conference of the North","issued":{"date-parts":[[2019]]},"language":"en","page":"729-734","publisher":"Association for Computational Linguistics","publisher-place":"Minneapolis, Minnesota","source":"DOI.org (Crossref)","title":"Better Modeling of Incomplete Annotations for Named Entity Recognition","type":"paper-conference","URL":"http://aclweb.org/anthology/N19-1079"},
  {"id":"joshiWordEmbeddingsLow2019","accessed":{"date-parts":[[2022,12,5]]},"author":[{"family":"Joshi","given":"Ishani"},{"family":"Koringa","given":"Purvi"},{"family":"Mitra","given":"Suman"}],"citation-key":"joshiWordEmbeddingsLow2019","container-title":"2019 International Conference on Document Analysis and Recognition Workshops (ICDARW)","DOI":"10.1109/ICDARW.2019.40090","event-place":"Sydney, Australia","event-title":"2019 International Conference on Document Analysis and Recognition Workshops (ICDARW)","ISBN":"978-1-72815-054-3","issued":{"date-parts":[[2019,9]]},"page":"110-115","publisher":"IEEE","publisher-place":"Sydney, Australia","source":"DOI.org (Crossref)","title":"Word Embeddings in Low Resource Gujarati Language","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/8893052/"},
  {"id":"oyewusiNaijaNERComprehensiveNamed2021","abstract":"Most of the common applications of Named Entity Recognition (NER) is on English and other highly available languages. In this work, we present our findings on Named Entity Recognition for 5 Nigerian Languages (Nigerian English, Nigerian Pidgin English, Igbo, Yoruba and Hausa). These languages are considered low-resourced, and very little openly available Natural Language Processing work has been done in most of them. In this work, individual NER models were trained and metrics recorded for each of the languages. We also worked on a combined model that can handle Named Entity Recognition (NER) for any of the five languages. The combined model works well for Named Entity Recognition(NER) on each of the languages and with better performance compared to individual NER models trained specifically on annotated data for the specific language. The aim of this work is to share our learning on how information extraction using Named Entity Recognition can be optimized for the listed Nigerian Languages for inclusion, ease of deployment in production and reusability of models. Models developed during this project are available on GitHub https://git.io/JY0kk and an interactive web app https://nigner.herokuapp.com/.","accessed":{"date-parts":[[2022,12,7]]},"author":[{"family":"Oyewusi","given":"Wuraola Fisayo"},{"family":"Adekanmbi","given":"Olubayo"},{"family":"Okoh","given":"Ifeoma"},{"family":"Onuigwe","given":"Vitus"},{"family":"Salami","given":"Mary Idera"},{"family":"Osakuade","given":"Opeyemi"},{"family":"Ibejih","given":"Sharon"},{"family":"Musa","given":"Usman Abdullahi"}],"citation-key":"oyewusiNaijaNERComprehensiveNamed2021","issued":{"date-parts":[[2021,3,30]]},"number":"arXiv:2105.00810","publisher":"arXiv","source":"arXiv.org","title":"NaijaNER : Comprehensive Named Entity Recognition for 5 Nigerian Languages","title-short":"NaijaNER","type":"article","URL":"http://arxiv.org/abs/2105.00810"}
]
